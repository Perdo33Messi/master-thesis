# Robotic Arm Navigation and Obstacle Avoidance Based on Deep Reinforcement Learning and Enhanced Model Predictive Path Integral Control

This repository contains the code implementation of my Master's thesis at the Technical University of Munich (TUM), titled  
**_Robotic Arm Navigation and Obstacle Avoidance Based on Deep Reinforcement Learning and Enhanced Model Predictive Path Integral Control_**.

The project addresses how to achieve **safe, stable, and efficient** motion control for robotic arms facing challenges such as dynamic obstacles, local minima traps, and sparse rewards. By combining deep reinforcement learning with a GPU-accelerated enhanced MPPI controller, it delivers a robust and real-time navigation system.



## ğŸ“Œ Project Overview

This research proposes a hybrid method that integrates Deep Reinforcement Learning (DRL) with an Enhanced MPPI controller:

- ğŸ§  **High-level policy** is generated by an HGG-based RL network (HER + HGG + EBP) for global path planning.
- ğŸš¦ **Low-level control** is handled by a GPU-accelerated MPPI controller, which samples multiple trajectories and computes a weighted average to generate optimal control actions.
- ğŸ”¢ **Tensorized optimization** parallelizes the MPPI sampling and evaluation process as matrix operations on GPU for significant speed-up.
- ğŸ”„ **Annealing mechanism** dynamically adjusts the sampling covariance, gradually transitioning from exploration to convergence.

This method was evaluated in **7 MuJoCo simulation environments**, featuring static and dynamic obstacles, object lifting, high-dimensional goals, and multiple local minima traps. Results show performance improvements in safety, efficiency, and real-time responsiveness over baselines such as exHGG, MPPI, and HGG-MPPI.



## ğŸ› ï¸ Tech Stack

- **Simulation**: MuJoCo + OpenAI Gym  
- **Reinforcement Learning**: DDPG + HER + HGG + EBP  
- **Control Algorithm**: Enhanced MPPI (GPU parallelism + covariance annealing)  
- **Robot Model**: Franka Emika Panda (simulation)



## ğŸ“ Project Structure

```bash
HGG-Enhanced MPPI/
â”‚
â”œâ”€â”€ algorithm/          # Core DDPG algorithm components
â”œâ”€â”€ learner/            # RL trainer (simulation)
â”œâ”€â”€ learner_real/       # RL trainer (real robot)
â”‚
â”œâ”€â”€ mppi/               # GPU-accelerated MPPI (simulation)
â”œâ”€â”€ mppi_real/          # MPPI for real robot
â”‚
â”œâ”€â”€ envs/               # Base environments
â”œâ”€â”€ env_ext/            # Extended environments (dynamic obstacles, doors)
â”œâ”€â”€ envs_real/          # Real robot environment setup
â”‚
â”œâ”€â”€ policies/           # Robotic arm policy collection (simulation)
â”œâ”€â”€ policies_real/      # Robotic arm policy collection (real robot)
â”‚
â”œâ”€â”€ utils2/             # Utility modules
â”‚
â”œâ”€â”€ play.py             # Simulation demo entry
â”œâ”€â”€ play_real.py        # Real robot demo entry
â”œâ”€â”€ train2.py           # RL training script
â”œâ”€â”€ test.py             # Test script (simulation)
â”œâ”€â”€ test_real.py        # Test script (real robot)
â”‚
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # Project description

```




## âš™ï¸ System Requirements

This framework was developed and tested on a Dell **XPS Desktop 8960** with the following specs:

- CPU: 13th Gen Intel(R) Core(TM) i7-13700  
- RAM: 64 GB  
- GPU: NVIDIA GeForce RTX 4060 Ti  
- OS: Ubuntu 24.04.1 LTS  


Use the `requirements.txt` file with `pyenv` to install all required dependencies.


ğŸ§ª **Create Virtual Environment with pyenv**

```bash
pyenv virtualenv 3.7.16 <env_name>
```

ğŸ”‹ **Activate the Virtual Environment**
```bash
pyenv activate <env_name>
```

ğŸ“¦ **Install Dependencies**
```bash
pip install -r requirements.txt
```

## ğŸµ **MuJoCo Setup**
Download MuJoCo 2.1 for Linux or macOS.

Extract the mujoco210 folder to: `~/.mujoco/mujoco210`.

If using a custom path, set the environment variable `MUJOCO_PY_MUJOCO_PATH`.

Set LD_LIBRARY_PATH:

```bash
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/.mujoco/mujoco210/bin
```

Make sure this environment variable is also set in your PyCharm run configurations.

## â–¶ï¸ **How to Run**
All run configurations are included in the PyCharm project.
It is highly recommended to use PyCharm as the IDE and run everything from there.

If you prefer using the terminal (not recommended), follow the steps below:

ğŸ§  **Train the RL Agent** (Choose one of the following environments)
- `FrankaFetchPickDynObstaclesEnv-v1`

- `FrankaFetchPickDynLiftedObstaclesEnv-v1`

- `FrankaFetchPickDynDoorObstaclesEnv-v1`

- `FrankaFetchPick3DTargetObstacle-v1`

- `FrankaFetchPickDynFrontVStaticGrooveEnv-v1`

- `FrankaFetchPickDynFrontUStaticGrooveEnv-v1`

- `FrankaFetchPickDynRearVStaticGrooveEnv-v1`

```bash
python train2.py --alg ddpg2 --epochs 20 --env=FrankaFetchPickDynObstaclesEnv-v1 --reward_min -10 --goal mpc
```

ğŸš€ **Run MPPI-HGG Hybrid Policy**
```bash
python play.py --env FrankaFetchPickDynObstaclesEnv-v1 \
               --play_path log/ddpg2-FrankaFetchPickDynObstaclesEnv-v1-hgg/ \
               --play_epoch 19 \
               --goal mpc \
               --play_policy MPPIRLPolicy \
               --timesteps 1000 \
               --env_n_substeps 5
```


To switch environments, modify the `--env` and `--play_path` parameters.

To compare against other algorithms (e.g., pure MPPI, HGG), change `--play_policy` to:

- MPPIPolicy

- RLPolicy

To load a different DDPG policy, change `--play_epoch`.

## ğŸ”­ Future Directions
HGG-Enhanced MPPI has shown promising results in simulation. Future work may explore:

- Online Adaptation: Real-time updates of dynamics and control parameters to adapt to changes like payload or external disturbances.

- Observation Uncertainty: Incorporate state estimation and confidence modeling to improve robustness in noisy or partially observable environments.

- Self-Collision and Advanced Constraints: Add self-collision detection to the cost function for safer operation in cluttered spaces.

- Sim-to-Real Transfer: Improve robustness to sensor noise and delays to enable real-world deployment.

- Vision/Language Model Integration: Use vision-language models to generate goals from images or text, enabling more flexible, generalized behavior.

Researchers and developers interested in RL and robot control are welcome to contribute. Feel free to reach out or open an issue / pull request ğŸ¤


## Note:
Due to the limited availability of Franka Emika Panda robotic arms in our lab, priority has recently been given to PhD students and postdocs. Despite applying three times, I was ultimately unable to secure a time slot. As a result, I was not able to conduct sim-to-real experiments on the real hardware for my thesis. The "real" part of the code still contains Patrick's implementation, which is the original HGG-MPPI code for real-robot experiments, kept here for reference.

<!-- ## ğŸ“œ License
This project is licensed under the MIT License. See the LICENSE file for details.

```bash
MIT License

Copyright (c) 2025 Jinjun Dong

Permission is hereby granted, free of charge, to any person obtaining a copy ...
...
``` -->

